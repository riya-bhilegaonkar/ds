---
title: "Final Report"
output: 
  html_document:
    toc: true
    toc_float: true
---

## Classification Model for Diabetes Risk

The Diabetes Risk Dataset contains 70,000 rows and 14 columns. The columns included a unique member ID (member_ID) number, age, height, weight, gender, systolic blood pressure (ap_hi), diastolic blood pressure (ap_lo), cholesterol, glucose (gluc), smoking (smoke), alcohol intake (alco), physical activity (active), body mass index (bmi) and at high risk of diabetes or not (diabetes). 6 of the potential predictor variables are categorical gender, cholesterol, glucose, smoke, alco and active, (along with the categorical response variable).



```{r, include = FALSE}
#Load Libraries
library(chunked)
library(tidyverse)
library(caret)
library(FNN)
library(doBy)
library(data.table)
library(h2o)
library(gtsummary)
library(e1071)
library(mice)
library(corrplot)

#for reproducibility
set.seed(4)
```



### Data Cleaning/Preprocessing
Upon visual inspection of the data, the first issue seemed to be the presence of missing values in the cholesterol column. After loading in the data variables were adjusted to their type of variable mentioned in the data dictionary. To allow for easier intepretation of the predictors in the case of day, height and gender the variables were adjusted to non-metric variables. Age is changed from days to year, height is changed from cm to inches and weight is changed from kilograms to pounds.

```{r, warning=FALSE}
#Data Preparation
diabetes_risk <- fread("data/diabetes_risk_data.csv") %>% rename(member_id = 1) %>% mutate(gender = recode(
    gender,
    "Male" = 0,
    "Female" = 1), gluc = na_if(gluc, 0)) %>% mutate(diabetes = as.factor(diabetes), gender = as.factor(gender), cholesterol = as.factor(cholesterol), gluc = as.factor(gluc), smoke = as.factor(smoke), active = as.factor(active), alco = as.factor(alco), age = as.integer(age/365), height = height/2.54, weight = weight*2.20462)
```

### Summary Statistics and Visualizations
```{r}
knitr::kable(summary(diabetes_risk[,-1]))
par(mfrow=c(3,2))
hist(diabetes_risk$age, main = "Histogram of Age", xlab = "Age", freq = FALSE)
hist(diabetes_risk$weight, main = "Histogram of Weight", xlab = "Weight", freq = FALSE)
hist(diabetes_risk$height, main = "Histogram of Height", xlab = "Height", freq = FALSE)
hist(diabetes_risk$ap_hi, main = "Histogram of Systolic BP", xlab = "Systolic BP", freq = FALSE)
hist(diabetes_risk$ap_lo, main = "Histogram of Diastolic BP", xlab = "Diastolic BP", freq = FALSE)

diabetes_risk <- diabetes_risk %>% mutate(ap_hi = replace(ap_hi, ap_hi>360, NA), ap_lo = replace(ap_lo, ap_lo>360, NA), ap_hi = replace(ap_hi, ap_hi<0, NA), ap_lo = replace(ap_lo, ap_lo<0, NA))
```

Upon viewing additional summary statistics I noticed more problems that were some missing values in the gluc variable and there were certain values coded with a 0. Additionally there were extremely high and low values for systolic and diastolic blood pressure, which I recoded as missing values. To combat the 0 values as it was not listed as a factor in the data dictionary, I recoded the 0 values form the gluc column to missing values.Then to fix the issue of missing values, I used the mice package to impute values with plausible data values drawn from distributions. This method was used as there were over 7000 missing values which is a large amount (these would be outlier values).

```{r, include = FALSE}
# Define arbitrary matrix with TRUE values when data is missing and FALSE otherwise
A <- is.na(diabetes_risk)
# Replace all the other columns which are not the one you want to impute (let say column 2)
A[,-c(6:9)] <- FALSE 
# Run the mice function
imputed <- mice(diabetes_risk, where = A)
imputed <- complete(imputed)
```

```{r}
knitr::kable(summary(imputed[,-1]))
par(mfrow=c(3,2))
hist(imputed$age, main = "Histogram of Age", xlab = "Age", freq = FALSE)
hist(imputed$weight, main = "Histogram of Weight", xlab = "Weight", freq = FALSE)
hist(imputed$height, main = "Histogram of Height", xlab = "Height", freq = FALSE)
hist(imputed$ap_hi, main = "Histogram of Systolic BP", xlab = "Systolic BP", freq = FALSE)
hist(imputed$ap_lo, main = "Histogram of Diastolic BP", xlab = "Diastolic BP", freq = FALSE)

```

From the summary statistics of the imputed dataset, there are no longer any missing values.

```{r, include}
#Data Partition
indexTrain <- createDataPartition(y = imputed$diabetes, p = 0.8, list = FALSE)
trainData <- imputed[indexTrain, ]
testData <- imputed[-indexTrain, ]

#for model training (remove id variable)
trainData <- trainData[,-1]
testData <- testData[-1]

#Checking for multicolinearity (#bmi and weight are correlated)
corrplot(cor(trainData[,c(1,3:6,13)]), method = "circle", type = "full")
```

In terms of checking for multicolineary, bmi and weight were highly positively correlated. Due to this reason I decided to remove the bmi variable from the predictor susbset (after testing models and seeing improvements in performance metrics with a smaller subset). 

### Final Model Choice

The classification model chosen that can accurately classify individuals as being risk for diabetes or not was a tested Gradient Boost Machines method. This machine learning method had the largest accuracy and AOC value. 

GBM:
Gradient boosting trains models in a gradual, additive and sequential manner (in terms of decision trees: each tree is grown using information from previously grown trees). In boosting, we only use the original data, and do not draw any random samples. The trees are grown successively, using a “slow” learning approach: each new tree is fit to the signal that is left over from the earlier trees, and shrunken down before it is used.

### Model Performance

| Metrics  | Logistic Regression | Gradient Boosting | Random Forest | Naive Bayes | Decision Tree |
|----------|---------------------|-------------------|---------------|-------------|---------------|
| MSE      | 0.2179              | 0.1764            | 0.1945        | 0.2725      | 0.2381        |
| RMSE     | 0.4668              | 0.4199            | 0.4409        | 0.5221      | 0.4879        |
| AUC      | 0.7115              | 0.8116            | 0.7858        | 0.6952      | 0.7822        |
| Accuracy | 0.6069              | 0.7330            | 0.7068        | 0.6194      | 0.7104        |


### Key Factors for Model Predictions

Model Summary: 

| Parameter       | Value |
|-----------------|-------|
| Number of Trees | 50    |
| Minimum Depth   | 5     |
| Mean Depth      | 5     |
| Maximum Depth   | 5     |
| Minimum Leaves  | 23    |
| Maximum Leaves  | 32    |
| Mean Leaves     | 30    |


* Generally more accurate compare to other modes,
* Train faster especially on larger datasets,
* Most of them provide support handling categorical features,
* Some of them handle missing values natively.
* Gradient boosting trees can be more accurate than random forests. Because we train them to correct each other’s errors, they’re capable of capturing complex patterns in the data.

### Limitations of Model

Disadvantages:

* Prone to overfitting, due to sensitivity to outliers.
* Models can be computationally expensive and take a long time to train.
* Difficult to interpret the final models.
* It is difficult to scale this algorithm as every estimator is dependent on its predecessor.

Solutions:

*  Regularization techniques eliminate the deterioration and reduce the overfitting effect. The number of gradient boosting iterations, M, is a well-known regularization parameter.

* In order to perform stoichastic gradient boosting, which is faster, a subsample of the training data is randomly selected (without replacement) from the entire training dataset for each iteration.

### Further Steps

If I had more time to work on the problem:
* I would spend more time on pre-processing the data, including additional time on outlier analysis. There were numerous data entry errors that could be more thoroughly combed through.
* I would test additional machine learning algorithms such as K-Nearest neighbours, Linear Discriminant Analysis and Quadratic Discriminant Analysis. Possibly using principle component analysis to choose better subsets of variables. 

### Business Impact

A GBM model could help assist in detecting members with high risk for diabetes and inherently lead to better predictions of unplanned hospital visits, increased risk of disease progression, increased need for specialist care and prescriptions for more expensive medication. Machine learning's effects on health insurance will enable both policyholders and insurers to save time and money. AI will undertake tedious tasks, freeing insurance professionals to concentrate on procedures that will enhance the experience of policyholders. Patients, hospitals, doctors, and insurance companies will gain from the GBM model's ability to complete tasks that are currently handled by people but are handled far more quickly and inexpensively by GBM. Ultimately it would be in Healthfirst's interest to create a classification model that assesses member diabetes risk status.
