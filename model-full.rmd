# %% [markdown]
# #Load Libraries
# library(chunked)
# library(tidyverse)
# library(caret)
# library(FNN)
# library(doBy)
# library(data.table)
# library(h2o)
# library(gtsummary)
# library(e1071)
# library(mice)
# library(kernlab)
# library(corrplot)
# 
# #for reproducibility
# set.seed(4)

# %% [code] {"execution":{"iopub.status.busy":"2023-02-25T19:50:31.701162Z","iopub.execute_input":"2023-02-25T19:50:31.702769Z","iopub.status.idle":"2023-02-25T19:50:31.734669Z"}}
#Install Libraries
#Load Libraries
library(tidyverse)
library(caret)
library(FNN)
library(doBy)
library(data.table)
library(h2o)
library(gtsummary)
library(e1071)
library(mice)
library(kernlab)
library(corrplot)

#for reproducibility
set.seed(4)


#Data Preparation
diabetes_risk <- fread('/kaggle/input/diabetes-risk/diabetes_risk_data.csv') %>% rename(member_id = 1) %>% mutate(gender = recode(
    gender,
    "Male" = 0,
    "Female" = 1), gluc = na_if(gluc, 0)) %>% mutate(diabetes = as.factor(diabetes), gender = as.factor(gender), cholesterol = as.factor(cholesterol), gluc = as.factor(gluc), smoke = as.factor(smoke), active = as.factor(active), alco = as.factor(alco), age = as.integer(age/365), height = height/2.54, weight = weight*2.20462)

knitr::kable(summary(diabetes_risk[,-1]))

# Define arbitrary matrix with TRUE values when data is missing and FALSE otherwise
A <- is.na(diabetes_risk)
# Replace all the other columns which are not the one you want to impute (let say column 2)
A[,-c(8:9)] <- FALSE 
# Run the mice function
imputed <- mice(diabetes_risk, where = A)
imputed <- complete(imputed)
View(imputed)

knitr::kable(summary(imputed[,-1]))

#Data Partition
indexTrain <- createDataPartition(y = imputed$diabetes, p = 0.8, list = FALSE)
trainData <- imputed[indexTrain, ]
testData <- imputed[-indexTrain, ]

#for model training (remove id variable)
trainData <- trainData[,-1]
testData <- testData[-1]

#checking for outliers 
cooksd <- cooks.distance(glm(diabetes ~ ., 
                             family = "binomial", 
                             data = trainData))

plot(cooksd, 
     pch="*", 
     cex=2, 
     main="Influential Obs by Cooks distance")  
abline(h = 4*mean(cooksd, na.rm=T), col="red")

outliers <- as.data.frame(rownames(trainData[cooksd > 4*mean(cooksd, na.rm=T), ]))
nrow(outliers)

#Checking for multicolinearity
corrplot(cor(trainData[,c(1,3:6,13)]), method = "circle", type = "full")

#Model Training with SVM
letter_classifier <- ksvm(diabetes ~ ., data = trainData,kernel = "vanilladot")
letter_classifier

#predictions:
letter_predictions <- predict(letter_classifier, testData)

#Check the accuracy:
caret::confusionMatrix(letter_predictions,testData$diabetes)
agreement <- letter_predictions == testData$diabetes
prop.table(table(agreement))

#Model Training with KNN
#kGrid <- expand.grid(k = seq(from = 1, to = 40, by = 1))
#fit.knn <- train(diabetes~., data = trainData,
         #       method = "knn",
         #       trControl = trainControl(method = "cv", number = 10),
         #       tuneGrid = kGrid)

#for h2o package
localH2O <- h2o.init(nthreads = -1)

#data as h2o cluster
train.h2o <- as.h2o(trainData)
test.h2o <- as.h2o(testData)

#dependent variable (Purchase)
y.dep <- 12

#independent variables (dropping ID variables)
x.indep <- c(1:11,13)

#Model Training with Random Forest
rforest.model <- h2o.randomForest(y=y.dep, x=x.indep, training_frame = train.h2o, ntrees = 1000, mtries = 3, max_depth = 4, seed = 4)

#Performance Metrics
h2o.performance(rforest.model, newdata = test.h2o)

#Model Training with Logistic Regression
logit.model <- h2o.glm( x = x.indep,
                                y = y.dep, 
                               training_frame = train.h2o, 
                               seed = 4,
                               family = "binomial",
                               lambda_search = TRUE,
                               alpha = 0.5, 
                               nfolds = 5 )

#Model Coefficients
h2o.coef(logit.model)

#Performance Metrics
h2o.performance(logit.model, newdata = test.h2o)

#Model Training with Naive Bayes
naive.h2o <- h2o.naiveBayes(x = x.indep,
                          y = y.dep,
                          training_frame = train.h2o,
                          laplace = 0,
                          nfolds = 5,
                          seed = 4)

#Performance Metrics
h2o.performance(naive.h2o, newdata = test.h2o)


# Eval performance:
#h2o.performance(svm_model, newdata = test.h2o)

tree.model = h2o.gbm(x = x.indep, 
                     y = y.dep, 
                        training_frame = train.h2o,
                        ntrees = 1, min_rows = 1, 
                        sample_rate = 1,            
                        col_sample_rate = 1,
                        max_depth = 5,
                        seed = 4)


#Performance Metrics
h2o.performance(tree.model, newdata = test.h2o)



# %% [markdown]
# 