---
title: "Tested Classification Models"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r}
#Load Libraries
library(chunked)
library(tidyverse)
library(caret)
library(FNN)
library(doBy)
library(data.table)
library(h2o)
library(gtsummary)
library(e1071)
library(mice)
library(corrplot)

#for reproducibility
set.seed(4)
```

```{r}
#Data Preparation
diabetes_risk <- fread("data/diabetes_risk_data.csv") %>% rename(member_id = 1) %>% mutate(gender = recode(
    gender,
    "Male" = 0,
    "Female" = 1), gluc = na_if(gluc, 0)) %>% mutate(diabetes = as.factor(diabetes), gender = as.factor(gender), cholesterol = as.factor(cholesterol), gluc = as.factor(gluc), smoke = as.factor(smoke), active = as.factor(active), alco = as.factor(alco), age = as.integer(age/365), height = height/2.54, weight = weight*2.20462)

knitr::kable(summary(diabetes_risk[,-1]))

# Define arbitrary matrix with TRUE values when data is missing and FALSE otherwise
A <- is.na(diabetes_risk)
# Replace all the other columns which are not the one you want to impute (let say column 2)
A[,-c(8:9)] <- FALSE 
# Run the mice function
imputed <- mice(diabetes_risk, where = A)
imputed <- complete(imputed)

knitr::kable(summary(imputed[,-1]))

#Data Partition
indexTrain <- createDataPartition(y = imputed$diabetes, p = 0.8, list = FALSE)
trainData <- imputed[indexTrain, ]
testData <- imputed[-indexTrain, ]

#for model training (remove id variable)
trainData <- trainData[,-1]
testData <- testData[-1]

#checking for outliers 
cooksd <- cooks.distance(glm(diabetes ~ ., 
                             family = "binomial", 
                             data = trainData))

plot(cooksd, 
     pch="*", 
     cex=2, 
     main="Influential Obs by Cooks distance")  
abline(h = 4*mean(cooksd, na.rm=T), col="red")

outliers <- as.data.frame(rownames(trainData[cooksd > 4*mean(cooksd, na.rm=T), ]))
nrow(outliers)

#Checking for multicolinearity (#bmi and weight are correlated)
corrplot(cor(trainData[,c(1,3:6,13)]), method = "circle", type = "full")
```

```{r}
#for h2o package
localH2O <- h2o.init(nthreads = -1)

#data as h2o cluster
train.h2o <- as.h2o(trainData)
test.h2o <- as.h2o(testData)

#dependent variable (Purchase)
y.dep <- 12

#independent variables 
x.indep <- c(1:11,13)
```



```{r}
#Model Training with Random Forest
rforest.model <- h2o.randomForest(y=y.dep, x=x.indep, training_frame = train.h2o, ntrees = 1000, mtries = 3, max_depth = 4, seed = 4)

#Performance Metrics
h2o.performance(rforest.model, newdata = test.h2o)
```

```{r}
#Model Training with Logistic Regression
logit.model <- h2o.glm( x = x.indep,
                                y = y.dep, 
                               training_frame = train.h2o, 
                               seed = 4,
                               family = "binomial",
                               lambda_search = TRUE,
                               alpha = 0.5, 
                               nfolds = 5 )

#Model Coefficients
h2o.coef(logit.model)

#Performance Metrics
h2o.performance(logit.model, newdata = test.h2o)
```

```{r}
#Model Training with Naive Bayes
naive.h2o <- h2o.naiveBayes(x = x.indep,
                          y = y.dep,
                          training_frame = train.h2o,
                          laplace = 0,
                          nfolds = 5,
                          seed = 4)

#Performance Metrics
h2o.performance(naive.h2o, newdata = test.h2o)
```

```{r}
tree.model = h2o.gbm(x = x.indep, 
                     y = y.dep, 
                        training_frame = train.h2o,
                        ntrees = 1, min_rows = 1, 
                        sample_rate = 1,            
                        col_sample_rate = 1,
                        max_depth = 5,
                        seed = 4)


#Performance Metrics
h2o.performance(tree.model, newdata = test.h2o)
```

